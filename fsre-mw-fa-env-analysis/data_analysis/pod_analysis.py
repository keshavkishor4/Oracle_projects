import sys,os,time,json
import pandas as pd
BASE_DIR = os.path.abspath(__file__ +"/../../")
sys.path.append(BASE_DIR)
from common import globalvariables
from octo_metric import octo_db
from dart_scraping import dart
from concurrent.futures import ThreadPoolExecutor
import traceback
from pprint import pprint

def pod_analysis(pod: str,start_time: str,end_time: str) -> None:
    '''
        Function name - pod_anaysis function will be used to provide pod performance analysis. 
        This Function will be calling to dart scraping and octo metric module to get the performance metric. 

    '''
    try:
        file_name="{0}/{1}_pod_dart_perf_data.json".format(globalvariables.exa_octo_data,pod)
        exa_metric,exadetails=octo_db.pod_octo_metric(pod,start_time,end_time)
        pod_dart=parse_exa_json(exa_metric, exadetails)
        # print(pod_dart)
        if  pod_dart:
            with ThreadPoolExecutor() as executer:
                exa_list=[dic["exa"] for dic in pod_dart]
                pods=[dic["pod"] for dic in pod_dart]
                st=[dic["start_time"] for dic in pod_dart]
                et=[dic["end_time"] for dic in pod_dart]
                results=executer.map(dart.get_pod_dart_data, pods,st,et)
                for count,start in enumerate(st):
                    for ct,spike in enumerate(exa_metric[exa_list[count]]["exa_high_cpu_spikes"]):
                        if spike["start_time"] == start:
                            for db_count, db in enumerate(spike["database"]):
                                if db["pod_name"] == pods[count]:
                                    exa_metric[exa_list[count]]["exa_high_cpu_spikes"][ct]["database"][db_count].update({"dart_data":[file.split("/")[-1] for file in next(results)]})
                                else:
                                    exa_metric[exa_list[count]]["exa_high_cpu_spikes"][ct]["database"][db_count].update({"dart_data":[]})
        # print(exa_metric)
        json_object = json.dumps(exa_metric, indent=4)
        with open(file_name,'w') as dart_data:
            dart_data.write(json_object) 
        return json_object              
    except Exception as e:
        traceback.print_exc()
        message="{0}Erro in fun - pod_anaysis {1}".format(globalvariables.RED,e)
        print(message)

def parse_exa_json(exa_metric: dict, exa_list: list) ->list:
    ''' 
        parse_exa_json fucntion will accept the exa_metric json generated by octo metric module and  return  the list for all exa spike start time and end time with podname.
        returned list data will be used to fetch respective dart mertric given time window. 

    '''
    try:
        dart_list=[]
        for exa in exa_list:
            for spike in exa_metric[exa]["exa_high_cpu_spikes"]:
                db_dict={}
                df=pd.DataFrame(spike["database"])
                try:
                    if "avg_db_cpu" in df.columns:
                        max_avg_cpu=df[["avg_db_cpu"]].max()["avg_db_cpu"]
                        # pprint(df)
                        # test=df[df["avg_db_cpu"] == max_avg_cpu]["pod_name"]
                        # pprint(test)
                        pod=df[df["avg_db_cpu"] == max_avg_cpu]["pod_name"].reset_index(drop=True)[0]
                        db_dict.update({"exa":exa,"pod":pod,"start_time":spike["start_time"],"end_time":spike["end_time"]})
                        dart_list.append(db_dict)
                except Exception as e:
                    message="error in panda avg_db_cpu parsing {0}".format(e)
                    print(message)
        return dart_list
    except Exception as e:
        traceback.print_exc()
        message="{0}Error in fun - parse_exa_json {1} ".format(globalvariables.RED,e)
        print(message)

def exa_view_frame(j_data):
    # with open(octo_file) as j_file:
    #     j_data=json.loads(j_file.read())
    try:
        dict_data=[]
        # print(type(j_data))
        for key,value in j_data.items():
            if not value['exa_high_cpu_spikes']:
                exa_dict={}
                exa_dict.update({"exa":key})
                exa_dict.update(value)
                exa_dict.update({"start_time":value["start_time"],"end_time":value["end_time"],"avg_cpu":value["avg_cpu"],"max_cpu":value["max_cpu"],"noisy_pod":'',"dart_url":'',"db_name":'',"pod_size":''})
                # node_no=key.split('.')[0].split('-')[1][-1]
                # exa_dict.update({"node_no":node_no})
                pods=[]
                for podname in value['databases']:
                    pods.append(podname['pod_name'])
                exa_dict.update({"pod_name":pods})
                del exa_dict['databases']
                del exa_dict['exa_high_cpu_spikes']
                del exa_dict['collection_time_stamp']
                del exa_dict['avg_swap']
                del exa_dict['max_swap']
                del exa_dict['avg_load']
                del exa_dict['max_load']
                # dict_data.update({key:[pod['pod_name'] for pod in j_data[key]['databases']]})
                # print(exa_dict)
                dict_data.append(exa_dict)
            else:
                time_spike=[]
                for pod in value['exa_high_cpu_spikes']:
                    spike={}
                    spike.update({"exa":key})
                    # node_no=key.split('.')[0].split('-')[1][-1]
                    # exa_dict.update({"node_no":node_no})
                    spike.update(pod)
                    pods=[]
                    noisy_pod=''
                    db_name=''
                    pod_size=''
                    dart_url=''
                    for podname in pod['database']:
                        pods.append(podname['pod_name'])
                        if podname.get('dart_data'):
                            noisy_pod=podname.get('pod_name')
                            db_name=podname.get('dbname')
                            pod_size=podname.get('pod_size')
                            dart_url= [ url for url in podname.get('dart_data') if "db_art_perf" in url][0]
                    spike.update({"pod_name":pods,"noisy_pod":noisy_pod,"dart_url":dart_url,"db_name":db_name,"pod_size":pod_size})
                    del spike['database']
                    time_spike.append(spike)
                dict_data=dict_data + time_spike
        # print(dict_data)
        df=pd.DataFrame(dict_data)
        return dict_data
    except Exception as e:
        traceback.print_exc()
        message="{0}Error in fun - exa_view_frame {1} ".format(globalvariables.RED,e)
        print(message)

def main():
    start=time.time()
    pod_analysis("EODR","2023-10-18T00:00:45","2023-10-19T23:40:45")
    end=time.time()
    print(end-start)

if __name__ == "__main__":
    main()
